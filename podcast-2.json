{"podcast_details": {"podcast_title": "The Stack Overflow Podcast", "episode_title": "Semantic search without the napalm grandma exploit", "episode_image": "https://image.simplecastcdn.com/images/f0fdf349-149b-42f9-95d0-8ddf72185776/02a60604-9d42-4ec4-a716-8d9a2942f79c/3000x3000/stack-overflow-podcast-1080x1080.jpg?aid=rss_feed", "episode_transcript": " Hello everybody. Welcome back to the Stack Overflow podcast, a place to talk all things software and technology. I am your host, Ben Popper, joined as I often am these days by my co-host, Kyle Mitt, senior software engineer here at Stack Overflow. We have a great conversation for y'all today. We're going to be talking a lot about the announcements we made recently and we are developers concerning the launch of Overflow AI and a bit about how we got here. So we have two great guests today, Michael and Alex. I'll let them introduce themselves in a second, tell you a bit about what they do here, and then we're going to dive into a conversation about what we've been building, how it's going to impact hopefully Stack Overflow, the public platform and Stack Overflow for teams. And then a bit of where all this is headed with some questions about some of the underlying technologies. So, I'm Alex. Welcome to the podcast. Thank you, Ben. Good to be here. Michael, why don't you kick it off? Tell people just really quickly who you are, what your role is here at Stack Overflow and a little bit about what your specific function was as we were thinking about launching Overflow AI and what all that is going to mean for Stack Overflow as a whole. Yep. So my name is Michael Forie. I'm the director of data science and data platform here at Stack Overflow. And for part of Overflow AI, I've been leading data scientists, data engineers, machine learning engineers, and trying to hold together the glue of all the data stuff that's flowing around, making sure that data is being stored properly, that it's being sent to the right machine learning models, that they're being built out in a timely fashion, and helping to ideate on what type of models we're going to need to support the different Overflow AI initiatives. Nice. And Alex, how about yourself? Hi, I'm Alex Warren. I'm a senior developer here at Stack Overflow. Just like Kyle, I'm also a tech lead. I'm tech lead of one of the teams that has been building bits of Overflow AI. Very cool. And what team are you the lead of, Alex? What is the focus of your team? The focus of my team. Okay. So my team is responsible for building a bunch of services and UIs for Overflow AI. So in particular, we are building an AI gateway. So that's an API that other teams across the company can use to connect to large language models. And we're also largely working on search APIs. So there's a semantic search API which we're working on, which we will talk about in a minute, I expect. And there's also the front end and back end of what we call the search experience. So the new search experience on Stack Overflow for teams, we are building the front and back end of that as well. Very cool. Yeah. So maybe let me set the stage here with my viewpoint on how folks are using LLMs at large and what the level of... There's a wired series. It's like explain at five levels of beginner level and then intermediate level and advanced and expert. And so in the world of LLMs, I think there's three different levels of depth and complexity you can take on in leveraging an LLM. The first is just we're going to solve it at the prompt level. We're going to use an off-the-shelf model. We're just going to... Everything off the shelf, but I'm going to prompt it and I'm going to get something back. And there's a lot of ways to leverage an LLM just by prompting it and doing it that way. I think the second level of depth you can get is using embeddings. So you put your own content there in a way that the LLM understands and then you can retrieve those. You can augment them. You can do certain stuff. And then the third level of intensity, I think there's a funnel and then most people are doing it at the prompt. Some people are doing embeddings. Very few people are building the actual models themselves. I was curious if that's the right framing, but if it is, then is there... I think in our project we did the first two. Is there a place you gravitated between those first two in terms of where you were picking up knowledge, where you were working on stuff? And then is there any interest in us pursuing the third? Are we ever going to look at a whole darn model? Or is it mostly the first two where we can take advantage of a lot of the ways that technology applies to software? Yeah. So I can speak to a bunch of that stuff. So just as a bit of background, I guess a lot of developers about six months ago, I hadn't touched anything to do with AI whatsoever. When we started on this project, we were starting from very rudimentary knowledge and we had to upskill really fast. So it's been a nice learning journey for me. I think what I have discovered actually is that, like you said, Kyle, at the prompt level, there's a lot of stuff you can do actually there. I would say what I have been focusing on is basically those first two things you talked about, prompt and embedding. So we definitely use embeddings. So we can generate embeddings for content that's on Stack Overflow and Stack Overflow for Teams. By generating those embeddings, we can do better search basically, which then lets us take those search results and we can use that in prompts and do some pretty cool stuff. Michael, aside from those two, do you think that we have a way we might get into the third piece of that, like our own models or our own training? Or does it make more sense for Stack Overflow to let other companies work on those foundational aspects or pick things off the shelf and focus instead, as Kyle was saying, on making sure our prompting and our embeddings are great so that people can make use of the incredible data that our community has and will continue to create? Yeah, I certainly do think that there's a future where Stack Overflow fine tunes our own large language model. There could be a handful of different business reasons why we would want to fine tune our own LLM. My personal desire is that absolutely I want to fine tune a large language model and I want to do it just for fun because I can, right? But when it comes down to like, how am I going to convince my boss to let me do this? I think that there's a lot of lift, as you say, in having better prompts going to a foundational large language model. There's a lot of lift in the retrieval aspect where you pull in your own content and you augment what the large language model is generating. There's even opportunity to game and A-B test different large language models. All of these things are much, much easier to do than fine tuning your own large language model. And so it's difficult for me to predict if we're going to start fine tuning our own for the enhancement of Overflow AI. Gotcha. So let's talk a little bit about what we have been working on and a little bit of what you shared in your fireside chat that you did together at We Are Developers. I think the beginning of this is a search comes in and then we're going to generate vector embeddings to look up in a vector database. At the end of that, we might rank and merge the results. We also have a lexical database. We shared a great article about this from David Haney and David Gibson, two other folks in our team. I'll link to that in the show notes. But let's sort of start at the beginning of that journey. How did we go about thinking about a vector database? Let's talk a little bit about that, how we would have constructed it. And then from there, let's move to sort of our approach to embeddings and how that's working at the moment. Yeah. So we do have this rather in-depth architecture. And I want to preface this. There are some members of our audience who might not be fully aware of what you can do with large language models. The first approach that anyone should do, anyone listening to this should absolutely find access to a large language model and just start prompting and asking it questions and see what happens. And you'll very quickly find that it gives some really interesting information. Sometimes it's right, sometimes it's wrong. Sometimes you don't actually know the difference because you're asking for stuff outside of your depth. And here at Stack Overflow, one of the things that we wanted to do is we wanted to seek to deliver high quality information to our users and leverage the expertise of the contributors that we already have on our platform. And going against a straight large language model, irrespective of what that large language model is, we very quickly found that the quality that we wanted to deliver just wasn't there and the community involvement also wasn't there. And so we decided to leverage our own content to augment what large language models do. And so the very first step of that is we overhauled search. We overhauled our ability to discover content on StackOverflow.com. The traditional search solution is a lexical search. Think Elastis search, right? It looks at words and it says, oh, you search for the word run. I'm going to help you find the word run. I'm going to get fancy. I'm going to help you find the word running. You search for Python. I'm going to look for the word Python. Semantic search kind of takes us on its head and it says, oh, you search for Python. That's probably not a snake. That's probably a programming language. So I'm going to help you find programming languages about Python. You search for run, maybe it's dog running and it's this type of aspect, or maybe it's not dog running. Maybe it's computer running. So I'm going to help you find the concept of is the computer on and functioning. And so semantic search changes the content discovery paradigm that we have to work with and it helps us drive a great new capabilities with discovering content. Yeah. And for the explain it to me like I'm five, being the resident non-engineer in the house, it's been described to me as like a vector database. Think about it as a 3D space where these different embeddings are getting stored and they're, as you said, being grouped together based more on their meaning, like two related subjects or topics as opposed to keywords that you would have to group them together in the search. One of the things that you pointed out at the beginning that's really interesting is, yeah, anybody can run up against chat GPT. You can pretty easily connect to that API, but the quality of the results as the model itself will tell you, includes a lot of hallucinations because it's reading the entire internet and trying to guess what you want to hear in response. What we're doing instead is using the stack overflow database as our corpus and hoping that that will get us more accurate results. Kyle, I know you wanted to dig a little into this. I'll throw it over to you for the next question. Yeah, sure. So one of the ways that we take semantic search is via embeddings. That's like where we take the content, the gist, the smell of a particular set of text and we store it as a vector and we put it in a vector database. As someone who really only touches SQL, can you compare a vector database to a SQL database? Can I just select star from it? How do you touch it? Yeah. So let's talk about that a little bit. So the first is that many people think of a vector database as taking a string of words and storing it in a different database. That's not quite right. The first step is that you take the string of words and you convert it into numbers. You don't convert each word into a number. You take the entire string and you understand its semantic understanding, its language understanding, and you convert all of that into numbers. You might have a thousand different numbers and number one might mean something like, is this set of words about Python? Yes or no. And not really yes or no. It's on a scale of zero to a hundred percent. Is this about Python? And the next one would be, is this about Java on a scale of zero to a hundred? Is this about programming? Is this about petting your dog or so on and so forth? Rinse and repeat a thousand times. And so you have these thousand different numbers that get stored in your favorite data store. And I say data store, I didn't say database and I didn't say vector database. You can store these numbers however you please. You can store them in a hashed array in memory that disappears as soon as you turn your computer off if you so desire. And Kyle, honest to God, a year ago, this is what we were doing at Stack Overflow when we were testing this stuff out is we were just storing this in memory because why not? It got the job done. And for the longest time, there weren't a lot of vector databases out in the market for us to choose from. A lot of other people are storing their vector embeddings just in memory or in a text file or in a normal SQL database and it gets the job done. But this is where the SQL database or the in-memory solution starts to break down. We have tens of millions of questions. If we were to store all of that in memory, it would take up a lot, a lot of space. I don't know how much. I haven't run the calculations recently, but we would run out of space. We can't do that. We could store this in a normal SQL database in your favorite relational database. But here's the next step that you're going to do with a search. You're going to get a search query that comes in. You're going to convert this search query into your vectors. And so you've got a thousand numbers and then you want to search against your existing embeddings on all thousand different numbers. So Kyle, have you ever indexed a database on a couple of different columns? Sure have. Have you ever indexed a database on a thousand different columns? No. I've never done that either. I don't even know if you can. I haven't tried. I don't want to try. So that's basically what you have to do if you're going to turn a SQL database into a vector database. Disclaimer, Postgres is working on this. They have an add-in where you can turn a Postgres database into a vector database. I haven't actually tried it. But they're working on this. But one of the things that vector databases specialize in is they don't index on all 1000 different columns and allow you to search on all 1000 columns. And they use K nearest neighbor tricks and they cluster the different data into a bunch of different smaller clusters. So then you take a pretty good guess at, oh, here's this new embedding that comes in. I'm not going to search against all 10 or 50 or 100 million rows. I'm only going to search against a couple of different clusters. I'm going to narrow it down to my favorite clusters and then I'm going to search against those and I'm going to do it really, really, really fast. And there's a lot of different vector databases that have popped up over the last six months. And the field is changing drastically. Day over day, there's new advancements that are coming up. I'm pretty sure we talked about this in the blog post. I just wanted to get it out there that we chose Weaviate. Is there anything you can say about that? Why we chose that one? Why it works particularly well for us or why it's suited to our needs? Yeah. Weaviate does a couple of different things that we were looking for. One is it's self-hosted. We didn't need a self-hosted solution, but hosting that ourselves relieved a lot of different problems around security compliance. Makes sense. You know, making sure that the other place that hosts this data is good. Weaviate also comes with a vector database as a service option. So if we so chose in the future, we could ask Weaviate to host the vector database and we just pay them some money. They also make it easy for us to, the term isn't re-index. But you can imagine when you add or change a bunch of data inside Weaviate, inside the vector database, you have to put it back in the clusters that it's expecting. And so they have a pretty straightforward way of doing that. And so we want to update our data every so often. And Weaviate makes it simple to do that. And then it had a pretty good user community support. Right. And I think the last one that was mentioned is we leaned pretty heavily into PySpark for, I guess, some of this work and it needed to have that sort of native spark connection. So from the tech stack we chose, it was a good match. Right. Yes. Valid. Valid contribution. Yes. Valid contribution received. So we've talked a little bit about now, you know, vector databases and embeddings and some of the reasons we went. Can we talk a little bit about what is RAG? Why we would choose to lean into that and how we think it will help with the search results. Alex, maybe you want to take that one? Sure. Yeah. So I think a lot of people, when they are starting to play around with these large language models, jump straight into prompt engineering and you can get some cool results back when you ask an AI question. But what we're doing here is rather than using the knowledge that's in the large language model itself, is we want to use the knowledge that's either on public Stack Overflow itself or for our business customers in their Stack Overflow for Teams instance. Because obviously we want our customers to be able to ask questions of their data and that's data that the large language model doesn't even have. And even if it did, large language models are not very good at knowing where this data has come from. So one of the key problems that we're looking to solve on both public and on Teams is just that kind of trust aspect of like, where does the data come from? Can we be sure that this data is true? So the way that it works is if a user types in a question and say, they say, how do I add to a list in Python? What we can do is a semantic search like Michael was just describing and we can find the closest question that already exists in the particular data set that we're searching to that. We then fetch back the answers to that question and we've got various ways that we can choose which answers to pick out of the search results that come back. And then what we can do is we can take those answers, which we know where they've come from and then we can construct a new prompt that says something like, here's the context, here's a bunch of data I've got back from Stack Overflow. Please answer this user's question. How do I add to a list in Python? And so then the large language model is going to use hopefully just the data that we give it to answer the question. And we know where that data has come from so we can have more faith that that is the right answer and a safe answer rather than using what the large language model reckons to be the answer. So that's the key difference. What I really like about ResultsAugmented Generation Rang is that LMS kind of offer these two features. One is just kind of semantic understanding of text and also pretty impressively for just regular consumers, a relatively large repository of knowledge. And so they've offered these two features, but if we just lean on them for the knowledge repository, then we're prone to hallucinations, we're prone to all these other problems. And what Rack allows us to do is just take one of those features at a time. Just say, I just want the engine, I will bring my own knowledge repository here. I'm not going to lean on you for the knowledge repository. That seems like a really hard job to put on one piece of technology to know everything. All know the stuff that our people are asking about. One challenge that that sometimes presents though is this last mile problem where if a user searches, how do I make a button blue? And we have a Stack Overflow post that like semantically kind of similar to that, how do I make a button red? We're going to surface that post and say, oh, that's pretty darn similar. Probably what you're doing to send the background color on it in HTML or something. Here's how to style it. Here's how to use CSS. The answer is really darn close. But if we just summarize that result, it's probably going to answer a slightly different version. Do we have any ways of solving that kind of last mile problem of recontextualizing the provided answers that we know are good quality sources given the problem that the user is actually trying to face at this point in time? Do we do anything there to kind of take those results back and summarize them? Yeah. So there's a couple different ways that you can use Rack style generation. And one that we're using on public platform is to say, oh, this is the results that we got back from our search. You know, kind of combine these together and present them to the user. It's something else that we're toying around with. And we're doing some experiments right now is that we give the language model a little bit more leeway. And we say, hey, these are factual things. This is how you make a button red. It's a fact, right? But can you change this to address what the user actually asked for? And what we're getting here is it's supposed to address that last mile question of this is the specific problem that was addressed. Can you make some changes? What we want to investigate is does this introduce any extra risk or errors when you go and change what we're giving? Because we're giving you a little bit more leeway than what we had before. And Michael, that's kind of in the hidden prompt, right? That the user doesn't see. But when the user asks a question and it goes through our system, there's sort of a long prompt that we've written in there that, as you said, is instructing the model to try to give back an answer structured in a certain way. Some of the interesting things I think you noted is if you don't know the answer, and I'm not sure how it knows if it doesn't, but don't be afraid to say when you don't know or solve that last mile problem and try to synthesize a little bit to get back to sort of what they would want, change red to blue. And I think a really important point you made also is can you give this answer in a way that also includes the sources? Because one of the things we're sort of committed to with Overflow AI is continue to have attribution and recognition for the community members who provided the knowledge in the first place. So maybe talk a little bit about that hidden prompt layer. I have a hot take on hidden prompts. So I think we've taken on a lot of tech debt as an industry looking at LLMs. Let me back up and explain how. So when we prompt an LLM, right now there are kind of three roles that you can have. You can have a user role, which is the prompt. You can have an assistant role, which is the response based on that user prompt. But there's also in a lot of LLMs a system role that is not the assistant going back and forth. It's the system. It's the, you are a Stack Overflow GPT. You are trying to answer this problem. And I think those system prompts are under leveraged to say, here are the rules that you must abide by. Here's the rules of the game. Every single prompt that follows this should be scoped and defined within those. The system prompt is something that the user has no access to. One of the tech debt pieces is that these have not been prioritized very high by existing models. When you send a system prompt, it's very easily overridden by the user prompt. So it doesn't do things like prevent against jailbreaking. There's this arms race in the user prompt where we take the user prompt and we wrap it and we say, here's how I want you to respond to the user prompt. But the jailbreaking problem is somebody is going to say, ah, ignore the previous thing I said. And all of that happens within the user prompt. And it's kind of created this arms race within there. Whereas I think the long-term shelf stable solution for this is really strong system prompts where that gets to override individual instructions within the user prompt. So I think as an industry, that's where we're moving. I think the weights are being played with those systems in evolutions of LLM models where they will weight the system prompt tire. In the meantime, we have a lot of different back and forth on the user prompt and hidden prompts and trying to solve that issue of making sure we get out of the system what we want. Yeah, I agree. So these APIs are improving all the time. And when we were starting out with this project, so what you're just describing is usually exposed via, certainly on OpenAI, it's via the ChatCompletions API. But I think that wasn't even released until fairly recently because when we started, there was only the raw completions API, which is just literally string in and string out. There's no structure to that. So as soon as you start copying user data in there, then it's essentially just like your classic sort of SQL injection type thing. Like if you were building a PHP app in 1997 or something like that, there's no kind of concept of creating a template that then user data can be safely inserted into. So we're starting to see that now. As you say, we've still got problems of just being able to simply get the prompt to ignore the system level stuff. So I'm sort of hoping like you that the industry just moves forward because then actually a bunch of problems that we're facing will just kind of go away because people who are paid to solve those problems can solve them for us. And then we can just consume these new nice secure APIs. Yes, let's hope so. I mean, napalm granny has got to be one of my favorite things that I've learned over the last year as a heuristic for thinking about life. But I also have been seeing research coming out recently saying research on jailbreaking, saying to what degree is this an inevitable feature that's sort of baked in and something we can't really engineer our way out of. I guess one of the things that I wanted to ask each of you before we conclude is sort of like, in brief, we gave kind of a roadmap announcement that we are developers. And there's a lot of exciting stuff there now. Folks can go to the labs page and sign up to be alpha testers. Thinking about where we are now, what are you most excited for in the next six months to a year? What is it that you're working on now? And where do you hope will be as this stuff continues to sort of roll out to the public? Well, I'm excited for a lot of it. And this stuff is just changing all the time. As I say, over the last few months, we've seen new models come out, new APIs come out. This stuff is transforming. And the thing that's really exciting to me personally is just being able to ride this wave. We're seeing the industry transforming all around us. It's quite rare that I've seen a transformation like this come along where I've actually got to be building stuff with it at the same time as it's happening rather than catching up years later. Oh, what was that that happened a couple of years ago with, oh, yeah, I remember this and finally getting a chance to play with these new toys. So I don't know. I think for me, I don't even know what's going to happen next. This stuff is changing so rapidly. I don't even know what to look forward to. But I know that this stuff is changing a lot. So I'm just excited to be here for the ride. Alex, I think you make a great point there. It's sort of like this feels like, oh, I just happened to be someone who's working at an early internet company in 1996, or I just happened to be someone who's working at a mobile app company in 2007. And so it's just exciting to get to be part of something that's really rapidly changing and is sort of infusing itself throughout the whole industry. And I think Stack Overflow with our data set is really uniquely positioned to do something interesting here. So I agree. I feel kind of privileged to get to work on this stuff. Michael, how about yourself? What are you thinking about? What are you looking forward to in the next six to 12 months? And what do you hope people will get to see out of Stack Overflow? I think Alex put it really well. The industry is transforming in front of us and we get to be a part of this. I think that the future of Stack Overflow is going to be unrecognizable five years from now. I think that over the next six to 12 months, I think we're going to put into practice a lot of the stuff that we've been playing around with and testing. I think that we're also going to innovate and find new things. It's difficult for me to guess at what's going to happen 12 months from now. I like that. You won't recognize us. I will say reports of our death have been greatly exaggerated. By the time you hear this podcast, there will probably have been a blog post a bit about our traffic stats and what they really look like. So I hope you would join us on the journey because actually we have an opportunity for people to really interact with helping us shape the future of Stack Overflow, the platform in a way that hasn't been true in a long time. Trying out features, giving feedback, participating in research and all that good stuff. All right, everybody. As we always do this time of the show, we got to shout out a Stack Overflow user who came on and shared some knowledge, helped to save a question from the dustbin of history. Congrats to Push Pendra on your Lifeboat badge. Came in and saved a question with your answer. The question is, error, invalid post back or call back argument. Not really sure exactly what the question means, but if you had issues with post back or call back, Pendra's got an answer for you, has earned themselves a Lifeboat badge and helped over 30,000 people. So appreciate you contributing some knowledge. The AI will make the most of it in the future. I am Ben Popper. I'm the Director of Content here at Stack Overflow. You can always find me on X, I hate saying that, at Ben Popper. You can always email us, podcast at Stack Overflow with questions or suggestions. And if you like the show, leave us a rating and a review. Really helps. I am Kyle Matofsky. You can find me on Twitter, still going to call Twitter, at KyleMitBTV. And as always, you can find me at StackOverflow at user ID 1366033. Alex Warren, Senior Developer. If you want to find me, actually, no, I'm not on social media these days, so you can't. My name is Michael Forre, Director of Data Science and Data Platform. You can find me on LinkedIn. Thanks for listening, everybody. And we will talk to you soon."}, "podcast_summary": "In this Stack Overflow podcast episode, the hosts discuss the recent announcement of Overflow AI and its impact on Stack Overflow. They are joined by guests Michael and Alex, who provide insights into their roles in developing Overflow AI. They explain the use of vector databases and embeddings in improving search results, as well as the importance of leveraging existing knowledge from the Stack Overflow community. The hosts also touch on the challenges of fine-tuning large language models and the potential for future advancements in the field. Overall, they express excitement about being part of the rapidly evolving AI industry and the future of Stack Overflow.", "podcast_guest": "Today on the Stack Overflow podcast, our hosts, Ben and Kyle, are joined by two guests, Michael and Alex, to discuss the recent launch of Overflow AI and its impact on Stack Overflow. Michael, as the Director of Data Science and Data Platform, led the team of data scientists, engineers, and machine learning experts responsible for building Overflow AI. Alex, a Senior Developer and Tech Lead, worked on building the AI gateway and search APIs for Overflow AI. They discussed the use of vector databases to store and retrieve vector embeddings, and how semantic search is changing the content discovery paradigm on Stack Overflow. They also touched on the concept of Results Augmented Generation (RAG) , which leverages the Stack Overflow knowledge base to provide more accurate and trustworthy answers. Overall, they expressed enthusiasm for the rapidly evolving field of AI and its potential to transform Stack Overflow in the coming years.", "podcast_highlights": "The main points in the podcast transcript are as follows:\n\n- The podcast introduces the topic of Overflow AI and its impact on Stack Overflow.\n- The hosts are joined by two guests, Michael and Alex, who provide insights into their roles in the development of Overflow AI.\n- They discuss the use of vector databases and embeddings to improve search results and the challenges involved in implementing this technology.\n- The conversation also touches on the use of large language models (LLMs) and the benefits of fine-tuning models for specific purposes.\n- The hosts and guests explore the concept of Results-Augmented Generation (RAG) and how it can enhance the accuracy and reliability of AI-generated answers.\n- They discuss potential solutions to the \"last mile problem\" of recontextualizing AI-generated answers to better match user queries.\n- The guests highlight the rapid evolution of AI technology and express excitement about being involved in the industry at this transformative time.\n- They emphasize the importance of community feedback and engagement in shaping the future of Stack Overflow and its AI initiatives."}